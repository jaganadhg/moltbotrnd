# =============================================================================
# Dev Container Compose — OpenClaw + Ollama for VS Code Dev Containers
# =============================================================================
# This file orchestrates three services:
#   1. devcontainer — VS Code attaches here (workspace + admin tools)
#   2. ollama       — Local LLM inference (phi4-mini)
#   3. openclaw     — Gateway service
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # Dev Container — VS Code workspace
  # ---------------------------------------------------------------------------
  devcontainer:
    image: docker.io/node:22-bookworm
    container_name: openclaw-devcontainer
    volumes:
      - ..:/workspace:cached
      - devcontainer_home:/home/node
    working_dir: /workspace
    user: node
    environment:
      - HOME=/home/node
      - TERM=xterm-256color
      - OPENCLAW_GATEWAY_TOKEN=${OPENCLAW_GATEWAY_TOKEN:-changeme}
      - OLLAMA_HOST=http://ollama:11434
    networks:
      - openclaw-internal
    # Keep container running for VS Code to attach
    command: sleep infinity
    depends_on:
      ollama:
        condition: service_healthy

  # ---------------------------------------------------------------------------
  # Ollama — Local LLM inference server
  # ---------------------------------------------------------------------------
  ollama:
    image: docker.io/ollama/ollama:latest
    container_name: openclaw-ollama-dev
    restart: unless-stopped
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 1G
    networks:
      - openclaw-internal
    volumes:
      - ollama_data:/root/.ollama
    tmpfs:
      - /tmp:size=512M,noexec,nosuid,nodev
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=1

  # ---------------------------------------------------------------------------
  # OpenClaw Gateway
  # ---------------------------------------------------------------------------
  openclaw:
    build:
      context: ../docker
      dockerfile: Dockerfile
    image: localhost/openclaw-gateway:local
    container_name: openclaw-gateway-dev
    restart: unless-stopped
    depends_on:
      ollama:
        condition: service_healthy
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    read_only: true
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "2.0"
        reservations:
          memory: 256M
    networks:
      - openclaw-internal
    ports:
      - "127.0.0.1:18789:18789"
    volumes:
      - openclaw_data:/home/node/.openclaw
      - ../docker/config/openclaw.json:/home/node/.openclaw/openclaw.json:ro
      - openclaw_workspace:/home/node/.openclaw/workspace
    tmpfs:
      - /tmp:size=256M,noexec,nosuid,nodev
      - /home/node/.openclaw/sessions:size=128M,noexec,nosuid,nodev
      - /home/node/.openclaw/logs:size=64M,noexec,nosuid,nodev
    environment:
      - NODE_ENV=production
      - HOME=/home/node
      - OPENCLAW_GATEWAY_TOKEN=${OPENCLAW_GATEWAY_TOKEN:-changeme}
      - OLLAMA_HOST=http://ollama:11434
    command:
      [
        "node", "dist/index.js", "gateway",
        "--bind", "lan",
        "--port", "18789",
        "--allow-unconfigured"
      ]

networks:
  openclaw-internal:
    driver: bridge

volumes:
  ollama_data:
    driver: local
  openclaw_data:
    driver: local
  openclaw_workspace:
    driver: local
  devcontainer_home:
    driver: local
